{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1TuRF7fUY7bsNnP728r85Ya2o2lKnBkcj",
      "authorship_tag": "ABX9TyN7Ak0AsPyd0i+SiWVdCQds",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bibinlouis703/NLP_Python_Blockchain/blob/main/NER_QA_finale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import random\n",
        "from spacy.training.example import Example\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Load your dataset\n",
        "file_path = 'sample.xlsx'\n",
        "data = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "qlH4StiOsWBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean column names and extract relevant data\n",
        "data.columns = data.columns.str.strip().str.replace('\\xa0', ' ')\n",
        "column_name = [\"How do you think blockchain technology could improve the financial integrity of NGOs? (Select all that apply)\",\n",
        "               \"How do you think NGOs can better educate the public about their Strategic Planning, Financial Data reporting and the technologies they use?\"]"
      ],
      "metadata": {
        "id": "F77vz3dY3vav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of entities and their labels to annotate\n",
        "entities_to_annotate = {\n",
        "    \"Blockchain\": \"TECHNOLOGY\",\n",
        "    \"transparency\": \"INTEGRITY\",\n",
        "    \"NGOs\": \"ORGANIZATION\",\n",
        "    \"smart contracts\": \"TECHNOLOGY\",\n",
        "    \"financial transactions\": \"FINANCE\",\n",
        "    \"strategic planning\": \"STRATEGY\",\n",
        "    \"social media\": \"PLATFORM\",\n",
        "    \"webinars\": \"EDUCATION\",\n",
        "    \"workshops\": \"EDUCATION\"\n",
        "}"
      ],
      "metadata": {
        "id": "T8rEJ0Db39zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to annotate a single sentence\n",
        "def annotate_text(sentence):\n",
        "    annotations = {\"entities\": []}\n",
        "    for entity, label in entities_to_annotate.items():\n",
        "        start = sentence.lower().find(entity.lower())  # Case-insensitive search\n",
        "        if start != -1:\n",
        "            end = start + len(entity)\n",
        "            annotations[\"entities\"].append((start, end, label))\n",
        "    return annotations"
      ],
      "metadata": {
        "id": "zwQC5AT23_d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "from spacy.training.example import Example\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "# Annotate each row in the column\n",
        "TRAINING_DATA = []\n",
        "for column in column_name:\n",
        "    for sentence in data[column].dropna():\n",
        "        annotations = annotate_text(sentence)\n",
        "        if annotations[\"entities\"]:  # Only keep rows with annotations\n",
        "            TRAINING_DATA.append((sentence, annotations))\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "random.shuffle(TRAINING_DATA)\n",
        "split_point = int(len(TRAINING_DATA) * 0.8)\n",
        "train_data = TRAINING_DATA[:split_point]\n",
        "test_data = TRAINING_DATA[split_point:]\n",
        "\n",
        "# Load spaCy model and exclude problematic components\n",
        "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"norm\", \"lexeme_norm\"])\n",
        "\n",
        "# Add NER to the pipeline if it's not already there\n",
        "if \"ner\" not in nlp.pipe_names:\n",
        "    ner = nlp.add_pipe(\"ner\", last=True)\n",
        "else:\n",
        "    ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "# Add labels for NER training\n",
        "for _, annotations in train_data:\n",
        "    for ent in annotations.get(\"entities\"):\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "# Create optimizer instead of initializing the pipeline (to avoid lookup issues)\n",
        "optimizer = nlp.create_optimizer()\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "# Initialize lists to store losses per iteration\n",
        "loss_per_iteration = []\n",
        "\n",
        "# Initialize lists to store accuracy metrics per iteration\n",
        "exact_match_accuracy_per_iteration = []\n",
        "partial_match_accuracy_per_iteration = []\n",
        "\n",
        "# Helper function to check for exact match\n",
        "def is_exact_match(pred_ent, true_ents):\n",
        "    for true_ent in true_ents:\n",
        "        if (pred_ent[0] == true_ent[0]) and (pred_ent[1] == true_ent[1]) and (pred_ent[2] == true_ent[2]):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Helper function to check for partial match\n",
        "def is_partial_match(pred_ent, true_ents):\n",
        "    for true_ent in true_ents:\n",
        "        if (pred_ent[2] == true_ent[2]) and (max(pred_ent[0], true_ent[0]) < min(pred_ent[1], true_ent[1])):  # Overlapping spans\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "CkVcIi053_7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for itn in range(10):  # Decreased iterations for quicker testing\n",
        "    random.shuffle(train_data)\n",
        "    losses = {}\n",
        "    total_entities = 0\n",
        "    exact_match_count = 0\n",
        "    partial_match_count = 0\n",
        "\n",
        "    batches = spacy.util.minibatch(train_data, size=4)  # Batch size set to 4\n",
        "\n",
        "    for batch in batches:\n",
        "        examples = []\n",
        "        for text, annotations in batch:\n",
        "            doc = nlp.make_doc(text)\n",
        "            example = Example.from_dict(doc, annotations)\n",
        "            examples.append(example)\n",
        "\n",
        "        # Update the model\n",
        "        nlp.update(examples, drop=0.1, losses=losses, sgd=optimizer)  # Lower dropout to 0.1\n",
        "    # Evaluate after each iteration on training data (can also use a separate test set)\n",
        "    for text, annotations in train_data:\n",
        "        doc = nlp(text)\n",
        "        pred_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "        true_ents = annotations[\"entities\"]\n",
        "\n",
        "        total_entities += len(true_ents)  # Count all true entities\n",
        "\n",
        "        # Print predictions for debugging\n",
        "        print(f\"Text: {text}\")\n",
        "        print(f\"True entities: {true_ents}\")\n",
        "        print(f\"Predicted entities: {pred_ents}\")\n",
        "        print()\n",
        "\n",
        "        # Exact match calculation\n",
        "        exact_match_count += len([pred_ent for pred_ent in pred_ents if is_exact_match(pred_ent, true_ents)])\n",
        "\n",
        "        # Partial match calculation\n",
        "        partial_match_count += len([pred_ent for pred_ent in pred_ents if is_partial_match(pred_ent, true_ents)])\n",
        "\n",
        "    # Calculate exact match accuracy and partial match accuracy for this iteration\n",
        "    exact_match_accuracy = exact_match_count / total_entities if total_entities > 0 else 0\n",
        "    partial_match_accuracy = partial_match_count / total_entities if total_entities > 0 else 0\n",
        "\n",
        "    exact_match_accuracy_per_iteration.append(exact_match_accuracy)\n",
        "    partial_match_accuracy_per_iteration.append(partial_match_accuracy)\n",
        "    # Capture the NER loss after each iteration\n",
        "    loss_per_iteration.append(losses.get(\"ner\", 0))\n",
        "    print(f\"Iteration {itn}, Losses: {losses}, Exact Match Accuracy: {exact_match_accuracy:.4f}, Partial Match Accuracy: {partial_match_accuracy:.4f}\")\n",
        "    # Log the accuracy\n",
        "    logger.info(f\"Iteration {itn+1}: Loss = {losses['ner']:.4f}, Exact Match Accuracy = {exact_match_accuracy:.4f}, Partial Match Accuracy = {partial_match_accuracy:.4f}\")\n",
        "\n",
        "# Plotting the Exact and Partial Match Accuracy over Iterations\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(exact_match_accuracy_per_iteration, marker='o', label='Exact Match Accuracy', color='b')\n",
        "plt.plot(partial_match_accuracy_per_iteration, marker='o', label='Partial Match Accuracy', color='r')\n",
        "plt.title('Exact and Partial Match Accuracy over Iterations')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the Loss vs Iteration graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(loss_per_iteration, marker='o', color='g', label='NER Loss')\n",
        "plt.title('Loss vs Iteration')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "d8NaNupD4b9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Test the Model\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Test on the 20% test set\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "# Track accuracy over iterations\n",
        "accuracy_scores = []\n",
        "iterations = []\n",
        "\n",
        "for text, annotations in test_data:\n",
        "    doc = nlp(text)\n",
        "    print(f\"\\nText: {text}\")\n",
        "    for ent in doc.ents:\n",
        "        print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
        "\n",
        "    # Collect true labels\n",
        "    true_entities = [ent[2] for ent in annotations[\"entities\"]]\n",
        "    predicted_entities = [ent.label_ for ent in doc.ents]\n",
        "\n",
        "    # Padding for mismatched lengths\n",
        "    max_len = max(len(true_entities), len(predicted_entities))\n",
        "\n",
        "    true_entities += [\"O\"] * (max_len - len(true_entities))  # Pad true labels\n",
        "    predicted_entities += [\"O\"] * (max_len - len(predicted_entities))  # Pad predicted labels\n",
        "\n",
        "    # Append to final lists\n",
        "    true_labels.extend(true_entities)\n",
        "    predicted_labels.extend(predicted_entities)\n",
        "        # Compute accuracy and store it\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    iterations.append(itn)\n"
      ],
      "metadata": {
        "id": "x8n4tcbP4i58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Evaluate the Model\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(true_labels, predicted_labels))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ],
      "metadata": {
        "id": "V4xX8UEG4tuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_accuracy(trainer):\n",
        "    epochs = []\n",
        "    accuracy = []\n",
        "\n",
        "    # Extract accuracy for each epoch\n",
        "    for log in trainer.state.log_history:\n",
        "        if \"eval_accuracy\" in log:\n",
        "            epochs.append(log['epoch'])\n",
        "            accuracy.append(log['eval_accuracy'])\n",
        "\n",
        "    # Plot accuracy over epochs\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(epochs, accuracy, marker='o', label='Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training Accuracy over Epochs')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bG5rbxZusNWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot graphs\n",
        "import matplotlib.pyplot as plt\n",
        "# Step 6: Evaluate the Model\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Final Accuracy: {accuracy}\")\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(true_labels, predicted_labels))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "# Visualize the confusion matrix using a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=sorted(set(true_labels)),\n",
        "            yticklabels=sorted(set(true_labels)))\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IHtwmDei4whO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "9iknLNIwtnyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy validate"
      ],
      "metadata": {
        "id": "s3QNIK-rtskZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the spacy model for embeddings\n",
        "nlp = spacy.load(\"en_core_web_md\")  # Using medium model for better embeddings\n",
        "# Define a function to calculate cosine similarity between question and sentence embeddings\n",
        "def calculate_similarity(question_embedding, sentence_embedding):\n",
        "    return cosine_similarity([question_embedding], [sentence_embedding])[0][0]\n",
        "\n",
        "# Function to find the best answer for a question based on NER-annotated sentences\n",
        "def get_best_answer(question, annotated_sentences):\n",
        "    question_embedding = nlp(question).vector  # Get embedding for the question\n",
        "    best_answer = None\n",
        "    best_score = -1  # Initialize with a very low score\n",
        "\n",
        "    # Loop through all sentences and their annotations\n",
        "    for sentence, annotations in annotated_sentences:\n",
        "        sentence_embedding = nlp(sentence).vector  # Get embedding for the sentence\n",
        "\n",
        "        # Compute similarity\n",
        "        similarity_score = calculate_similarity(question_embedding, sentence_embedding)\n",
        "\n",
        "        # If the similarity score is higher, update the best answer\n",
        "        if similarity_score > best_score:\n",
        "            best_score = similarity_score\n",
        "            best_answer = sentence\n",
        "\n",
        "    return best_answer\n",
        "\n",
        "# Example of using NER-annotated sentences (TRAINING_DATA is already created in previous steps)\n",
        "def answer_questions(questions, annotated_sentences):\n",
        "    answers = {}\n",
        "    for question in questions:\n",
        "        answer = get_best_answer(question, annotated_sentences)\n",
        "        answers[question] = answer\n",
        "    return answers"
      ],
      "metadata": {
        "id": "p11AolxlnoJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the annotated sentences (TRAINING_DATA already contains NER annotations)\n",
        "answers = answer_questions(column_name, TRAINING_DATA)\n",
        "\n",
        "# Print the answers\n",
        "for question, answer in answers.items():\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "7lCr3IT2o4pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "import networkx as nx\n",
        "\n",
        "# Example blockchain block structure\n",
        "class QA_Block:\n",
        "    def __init__(self, block_id, question, answer, entities, prev_hash=\"\"):\n",
        "        self.block_id = block_id\n",
        "        self.question = question\n",
        "        self.answer = answer\n",
        "        self.entities = entities\n",
        "        self.timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        self.prev_hash = prev_hash\n",
        "        self.block_hash = self.compute_hash()\n",
        "\n",
        "    def compute_hash(self):\n",
        "        block_string = f\"{self.block_id}{self.question}{self.answer}{self.entities}{self.timestamp}{self.prev_hash}\"\n",
        "        return hashlib.sha256(block_string.encode()).hexdigest()\n",
        "\n",
        "# Build the blockchain structure based on the QA results\n",
        "blocks = []\n",
        "prev_hash = \"\"\n",
        "\n",
        "for idx, (question, answer) in enumerate(answers.items()):\n",
        "    # Find the entities in the answer (from NER results)\n",
        "    doc = nlp(answer)\n",
        "    entities = set(ent.label_ for ent in doc.ents)  # Collect unique entity labels\n",
        "    block = QA_Block(block_id=idx+1, question=question, answer=answer, entities=entities, prev_hash=prev_hash)\n",
        "    blocks.append(block)\n",
        "    prev_hash = block.block_hash  # Update the previous hash\n",
        "\n",
        "# Create a knowledge graph using networkx\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes for each QA block and include its entities in the label\n",
        "for block in blocks:\n",
        "    G.add_node(block.block_id, label=f\"Block {block.block_id}\\n{block.entities}\")\n",
        "\n",
        "# Connect nodes that share entities\n",
        "for i, block1 in enumerate(blocks):\n",
        "    for j, block2 in enumerate(blocks):\n",
        "        if i != j and block1.entities & block2.entities:  # If they share at least one entity\n",
        "            G.add_edge(block1.block_id, block2.block_id)\n",
        "\n",
        "# Plot the graph\n",
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', labels={node: G.nodes[node]['label'] for node in G.nodes()})\n",
        "plt.title(\"QA Blockchain Knowledge Graph\")\n",
        "plt.show()\n",
        "\n",
        "# Print the blockchain structure\n",
        "for block in blocks:\n",
        "    print(f\"Block ID: {block.block_id}\")\n",
        "    print(f\"Question: {block.question}\")\n",
        "    print(f\"Answer: {block.answer}\")\n",
        "    print(f\"Entities: {block.entities}\")\n",
        "    print(f\"Timestamp: {block.timestamp}\")\n",
        "    print(f\"Previous Hash: {block.prev_hash}\")\n",
        "    print(f\"Current Block Hash: {block.block_hash}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "RSmTV4A40EMd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}